{
 "cells": [
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Data_cyb.json\", lines = True, orient = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "\n",
    "for i in df[\"annotation\"]:\n",
    "    rating.append(int(i[\"label\"][0]))\n",
    "    \n",
    "df[\"rating\"] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>content</th>\n",
       "      <th>extras</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      annotation  \\\n",
       "0  {'notes': '', 'label': ['1']}   \n",
       "1  {'notes': '', 'label': ['1']}   \n",
       "2  {'notes': '', 'label': ['1']}   \n",
       "3  {'notes': '', 'label': ['1']}   \n",
       "4  {'notes': '', 'label': ['1']}   \n",
       "\n",
       "                                             content  extras  rating  \n",
       "0                             Get fucking real dude.     NaN       1  \n",
       "1   She is as dirty as they come  and that crook ...     NaN       1  \n",
       "2   why did you fuck it up. I could do it all day...     NaN       1  \n",
       "3   Dude they dont finish enclosing the fucking s...     NaN       1  \n",
       "4   WTF are you talking about Men? No men thats n...     NaN       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>That is someone who does it from their heart. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Absolutely applaud your work to secure freedom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>You'll never learn it till you actually live i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nothing on the reinstatement of federal Capito...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Crickets</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  rating\n",
       "96   That is someone who does it from their heart. ...       1\n",
       "97   Absolutely applaud your work to secure freedom...       0\n",
       "98   You'll never learn it till you actually live i...       1\n",
       "99   Nothing on the reinstatement of federal Capito...       1\n",
       "100                                           Crickets       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"Test_Twitter_Comments.csv\")\n",
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1 = df[[\"content\", \"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df1,tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\tdgso\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
=======
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(new_df[\"content\"], new_df[\"rating\"], train_size = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(X)\n",
    "reviews_test_clean = preprocess_reviews(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdgso\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6629772416366123\n",
      "Accuracy for C=0.05: 0.7104837706752891\n",
      "Accuracy for C=0.25: 0.7530157940554657\n",
      "Accuracy for C=0.5: 0.7700534759358288\n",
      "Accuracy for C=1: 0.7859718940430295\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6591219997512747\n",
      "Accuracy for C=0.05: 0.7024001989802263\n",
      "Accuracy for C=0.25: 0.7505285412262156\n",
      "Accuracy for C=0.5: 0.7679393110309662\n",
      "Accuracy for C=1: 0.7818679268747668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_baseline, y, train_size = 0.5\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
<<<<<<< HEAD
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n"
=======
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words\n",
    "Removing Stop Words\n",
    "\n",
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’. We can usually remove these \n",
    "\n",
    "words without changing the semantics of a text "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdgso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "nltk.download('stopwords')"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 12,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 13,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.6734643123601094\n",
      "Accuracy for C=0.05: 0.7154936582939567\n",
      "Accuracy for C=0.25: 0.7704551106689879\n",
      "Accuracy for C=0.5: 0.7925889082317832\n",
      "Accuracy for C=1: 0.8099975130564536\n"
=======
      "Accuracy for C=0.01: 0.6761999502611291\n",
      "Accuracy for C=0.05: 0.7237005719970157\n",
      "Accuracy for C=0.25: 0.769211638895797\n",
      "Accuracy for C=0.5: 0.7933349912956976\n",
      "Accuracy for C=1: 0.8117383735389206\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "X = cv.transform(no_stop_words_train)\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "A common next step in text preprocessing is to normalize the words in your corpus by trying to convert all of the different forms of a given word into one. Two methods that exist for this are Stemming and Lemmatization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 14,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.6806764486446157\n",
      "Accuracy for C=0.05: 0.7182292961949763\n",
      "Accuracy for C=0.25: 0.7595125590649092\n",
      "Accuracy for C=0.5: 0.7833872171101716\n",
      "Accuracy for C=1: 0.7978114896791842\n"
=======
      "Accuracy for C=0.01: 0.682666003481721\n",
      "Accuracy for C=0.05: 0.7199701566774435\n",
      "Accuracy for C=0.25: 0.7684655558318826\n",
      "Accuracy for C=0.5: 0.785625466301915\n",
      "Accuracy for C=1: 0.8037801541904999\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "def get_stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "X = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
<<<<<<< HEAD
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n"
=======
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tdgso/nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tdgso/nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4e7248b1c616>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlemmatized_reviews_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemmatized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_train_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mlemmatized_reviews_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemmatized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_test_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4e7248b1c616>\u001b[0m in \u001b[0;36mget_lemmatized_text\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlemmatized_reviews_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemmatized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_train_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4e7248b1c616>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlemmatized_reviews_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemmatized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_train_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4e7248b1c616>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlemmatized_reviews_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemmatized_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_train_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tdgso/nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tdgso\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6781895050982343\n",
      "Accuracy for C=0.05: 0.727430987316588\n",
      "Accuracy for C=0.25: 0.7652325292215867\n",
      "Accuracy for C=0.5: 0.7838846058194479\n",
      "Accuracy for C=1: 0.7993036558070132\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
<<<<<<< HEAD
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_lemmatized = LogisticRegression(C=1)\n",
    "final_lemmatized.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_lemmatized.predict(X_test)))"
=======
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "Last time we used only single word features in our model, which we call 1-grams or unigrams. We can potentially add more predictive power to our model by adding two or three word sequences (bigrams or trigrams) as well. For example, if a review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review.\n",
    "The scikit-learn library makes this really easy to play around with. Just use the ngram_range argu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.7157423526485949\n",
      "Accuracy for C=0.05: 0.8264113404625715\n",
      "Accuracy for C=0.25: 0.860482467047998\n",
      "Accuracy for C=0.5: 0.8634667993036558\n",
      "Accuracy for C=1: 0.8664511315593136\n",
      "Final Accuracy: 0.9124595871673713\n"
=======
      "Accuracy for C=0.01: 0.7338970405371799\n",
      "Accuracy for C=0.05: 0.8331260880378015\n",
      "Accuracy for C=0.25: 0.8746580452623726\n",
      "Accuracy for C=0.5: 0.8796319323551356\n",
      "Accuracy for C=1: 0.8786371549365829\n",
      "Final Accuracy: 0.9156926137776672\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 4))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_ngram = LogisticRegression(C=1)\n",
    "final_ngram.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_ngram.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.659537428500373\n",
      "Accuracy for C=0.05: 0.7169858244217856\n",
      "Accuracy for C=0.25: 0.7610047251927381\n",
      "Accuracy for C=0.5: 0.7898532703307635\n",
      "Accuracy for C=1: 0.8114896791842825\n",
      "Final Accuracy: 0.7331509574732653\n"
=======
      "Accuracy for C=0.01: 0.6543148470529719\n",
      "Accuracy for C=0.05: 0.6980850534692863\n",
      "Accuracy for C=0.25: 0.7463317582690873\n",
      "Accuracy for C=0.5: 0.7731907485700075\n",
      "Accuracy for C=1: 0.7958219348420791\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wc_vectorizer = CountVectorizer(binary=False)\n",
    "wc_vectorizer.fit(reviews_train_clean)\n",
    "X = wc_vectorizer.transform(reviews_train_clean)\n",
    "X_test = wc_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75, \n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
<<<<<<< HEAD
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_wc = LogisticRegression(C=0.05)\n",
    "final_wc.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_wc.predict(X_test)))"
=======
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse\n",
    "\n",
    "document frequency) for each word, which is a weighting factor that we can use in place of binary or word count \n",
    "\n",
    "representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.6090524745088286\n",
      "Accuracy for C=0.05: 0.649589654314847\n",
      "Accuracy for C=0.25: 0.7207162397413579\n",
      "Accuracy for C=0.5: 0.7426013429495151\n",
      "Accuracy for C=1: 0.7704551106689879\n",
      "Final Accuracy: 0.7540412832628699\n"
=======
      "Accuracy for C=0.01: 0.6038298930614275\n",
      "Accuracy for C=0.05: 0.6391444914200448\n",
      "Accuracy for C=0.25: 0.7219597115145486\n",
      "Accuracy for C=0.5: 0.7463317582690873\n",
      "Accuracy for C=1: 0.7746829146978363\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
<<<<<<< HEAD
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n"
=======
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still has room to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Recall that linear classifiers tend to work well on very sparse datasets (like the one we have). Another algorithm that can produce great results with a quick training time are Support Vector Machines with a linear kernel.\n",
    "Here’s an example with an n-gram range from 1 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.01: 0.8271574235264859\n",
      "Accuracy for C=0.05: 0.8681919920417807\n",
      "Accuracy for C=0.25: 0.8684406863964188\n",
      "Accuracy for C=0.5: 0.8657050484953992\n",
      "Accuracy for C=1: 0.8632181049490176\n"
=======
      "Accuracy for C=0.01: 0.8182044267595125\n",
      "Accuracy for C=0.05: 0.8594876896294454\n",
      "Accuracy for C=0.25: 0.8552598856005968\n",
      "Accuracy for C=0.5: 0.852026858990301\n",
      "Accuracy for C=1: 0.8470529718975379\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Final Accuracy: 0.892066650087043\n"
=======
      "Final Accuracy: 0.8965431484705297\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "final_svm_ngram = LinearSVC(C=0.05)\n",
    "final_svm_ngram.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_svm_ngram.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 21,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": 22,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             content|rating|\n",
      "+--------------------+------+\n",
      "| Get fucking real...|     1|\n",
      "| She is as dirty ...|     1|\n",
      "| why did you fuck...|     1|\n",
      "| Dude they dont f...|     1|\n",
      "| WTF are you talk...|     1|\n",
      "|Ill save you the ...|     1|\n",
      "| Im dead serious....|     1|\n",
      "|...go absolutely ...|     1|\n",
      "|\"Lmao  im watchin...|     1|\n",
      "|LOL  no he said  ...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# Load in user_data.csv from S3 into a DataFrame\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"CsvReader\").getOrCreate()\n",
    "spark_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"C:\\Users\\ajkim\\Desktop\\Final Project\\final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             content|rating|\n",
      "+--------------------+------+\n",
      "| Get fucking real...|     1|\n",
      "| She is as dirty ...|     1|\n",
      "| why did you fuck...|     1|\n",
      "| Dude they dont f...|     1|\n",
      "| WTF are you talk...|     1|\n",
      "|Ill save you the ...|     1|\n",
      "| Im dead serious....|     1|\n",
      "|...go absolutely ...|     1|\n",
      "|\"Lmao  im watchin...|     1|\n",
      "|LOL  no he said  ...|     1|\n",
      "|truth on both cou...|     1|\n",
      "|   Shakespeare nerd!|     1|\n",
      "|you are SUCH a fu...|     1|\n",
      "|Heh. Fuck 'em WHE...|     1|\n",
      "|damn it i totally...|     1|\n",
      "|wow  damn I would...|     1|\n",
      "|nigga u geigh lma...|     1|\n",
      "|       that sucks :(|     1|\n",
      "|\"read that this m...|     1|\n",
      "|Unibroue 17 !!!! ...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark_df = spark_df.withColumn(\"rating1\", spark_df[\"rating\"].cast(IntegerType()))\n",
    "spark_df = spark_df.drop(spark_df.rating)\n",
    "spark_df = spark_df.withColumnRenamed(\"rating1\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------+\n",
      "|label|         review_text|review_length|\n",
      "+-----+--------------------+-------------+\n",
      "|    1| Get fucking real...|           23|\n",
      "|    1| She is as dirty ...|          130|\n",
      "|    1| why did you fuck...|          130|\n",
      "|    1| Dude they dont f...|          126|\n",
      "|    1| WTF are you talk...|           75|\n",
      "|    1|Ill save you the ...|          107|\n",
      "|    1| Im dead serious....|          130|\n",
      "|    1|...go absolutely ...|          130|\n",
      "|    1|\"Lmao  im watchin...|          132|\n",
      "|    1|LOL  no he said  ...|           76|\n",
      "|    1|truth on both cou...|          108|\n",
      "|    1|   Shakespeare nerd!|           17|\n",
      "|    1|you are SUCH a fu...|           27|\n",
      "|    1|Heh. Fuck 'em WHE...|           22|\n",
      "|    1|damn it i totally...|           34|\n",
      "|    1|wow  damn I would...|           44|\n",
      "|    1|nigga u geigh lma...|           45|\n",
      "|    1|       that sucks :(|           13|\n",
      "|    1|\"read that this m...|           79|\n",
      "|    1|Unibroue 17 !!!! ...|           43|\n",
      "+-----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, review_text: string, review_length: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, length\n",
    "review_df = spark_df.withColumnRenamed(\"rating\", \"label\")\\\n",
    "      .withColumnRenamed(\"content\", \"review_text\")\\\n",
    "      .select([\"label\", \"review_text\"])\n",
    "review_df = review_df.withColumn('review_length', length(review_df['review_text'])).dropna()\n",
    "review_df.cache()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 26,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Create all the features to the data set\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 83,
=======
   "execution_count": 27,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'review_length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 84,
=======
   "execution_count": 28,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 85,
=======
   "execution_count": 29,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(review_df)\n",
    "cleaned = cleaner.transform(review_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 87,
=======
   "execution_count": 30,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": 31,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|         review_text|review_length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
<<<<<<< HEAD
      "|    0|                  (:|            4|            [, , (:]|            [, , (:]|(262144,[165275,2...|(262144,[165275,2...|(262145,[165275,2...|[-62.382917087441...|[0.99998444852857...|       0.0|\n",
      "|    0|             *RAWR*?|            9|       [, , *rawr*?]|       [, , *rawr*?]|(262144,[249180,2...|(262144,[249180,2...|(262145,[249180,2...|[-144.63325572536...|[0.39245270585135...|       1.0|\n",
      "|    0|  *wearr hmmm hah...|           29|[, , *wearr, hmmm...|[, , *wearr, hmmm...|(262144,[18497,86...|(262144,[18497,86...|(262145,[18497,86...|[-506.35036968581...|[0.99999999999992...|       0.0|\n",
      "|    0|                   .|            3|             [, , .]|             [, , .]|(262144,[1536,249...|(262144,[1536,249...|(262145,[1536,249...|[-56.651701326422...|[0.99857054190243...|       0.0|\n",
      "|    0|                   .|            3|             [, , .]|             [, , .]|(262144,[1536,249...|(262144,[1536,249...|(262145,[1536,249...|[-56.651701326422...|[0.99857054190243...|       0.0|\n",
=======
      "|    0|                  ..|            4|            [, , ..]|            [, , ..]|(262144,[30445,24...|(262144,[30445,24...|(262145,[30445,24...|[-69.720217649278...|[0.49373838023583...|       1.0|\n",
      "|    0|  1  no 2  idk..?...|          234|[, , 1, , no, 2, ...|[, , 1, , 2, , id...|(262144,[4200,963...|(262144,[4200,963...|(262145,[4200,963...|[-2348.2707658471...|[1.0,7.0837981960...|       0.0|\n",
      "|    0|               10ish|            7|         [, , 10ish]|         [, , 10ish]|(262144,[173912,2...|(262144,[173912,2...|(262145,[173912,2...|[-142.87614982740...|[0.35535697681538...|       1.0|\n",
      "|    0|  1995 haha when ...|           85|[, , 1995, haha, ...|[, , 1995, haha, ...|(262144,[21020,22...|(262144,[21020,22...|(262145,[21020,22...|[-901.23438569949...|[1.0,4.9799952817...|       0.0|\n",
      "|    0|                2 :)|            6|         [, , 2, :)]|         [, , 2, :)]|(262144,[212053,2...|(262144,[212053,2...|(262145,[212053,2...|[-69.148494896892...|[0.99230003466935...|       0.0|\n",
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
      "+-----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": 32,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy of model at predicting reviews was: 0.805027\n"
=======
      "Accuracy of model at predicting reviews was: 0.804793\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 33,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 34,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy for C=0.04: 0.8184305434647432\n",
      "Accuracy for C=0.05: 0.8197985325208308\n",
      "Accuracy for C=0.06: 0.8193010819549807\n",
      "Accuracy for C=0.07: 0.8194254445964432\n",
      "Accuracy for C=0.08: 0.8179330928988932\n",
      "Accuracy for C=0.09: 0.8175600049745056\n",
      "Accuracy for C=0.1: 0.8163163785598806\n",
      "Accuracy for C=0.11: 0.8153214774281806\n",
      "Accuracy for C=0.12: 0.8161920159184181\n",
      "Accuracy for C=0.13: 0.8156945653525681\n",
      "Accuracy for C=0.14: 0.8151971147867181\n",
      "Accuracy for C=0.15: 0.8166894664842681\n",
      "Final Accuracy: 0.8522755533449391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
=======
      "Accuracy for C=0.04: 0.8183061808232807\n",
      "Accuracy for C=0.05: 0.8191767193135182\n",
      "Accuracy for C=0.06: 0.8206690710110682\n",
      "Accuracy for C=0.07: 0.8196741698793683\n",
      "Accuracy for C=0.08: 0.8194254445964432\n",
      "Accuracy for C=0.09: 0.8188036313891307\n",
      "Accuracy for C=0.1: 0.8181818181818182\n",
      "Accuracy for C=0.11: 0.8171869170501181\n",
      "Accuracy for C=0.12: 0.8166894664842681\n",
      "Accuracy for C=0.13: 0.8169381917671932\n",
      "Accuracy for C=0.14: 0.8166894664842681\n",
      "Accuracy for C=0.15: 0.8153214774281806\n"
     ]
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.5\n",
    ")\n",
    "\n",
    "ccc = []\n",
    "c_scores = []\n",
    "\n",
    "for c in [0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "    ccc.append(c)\n",
<<<<<<< HEAD
    "    c_scores.append(accuracy_score(y_val, svm.predict(X_val)))   \n",
    "                    \n",
    "final = LinearSVC(tol=.000001,C=0.01)\n",
    "final.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final.predict(X_test)))\n",
    "\n",
    "                    \n",
    "import matplotlib.pyplot as plt    \n",
    "                    \n",
    "plt.plot(ccc, c_scores)\n",
    "plt.show()\n",
    "        "
=======
    "    c_scores.append(accuracy_score(y_val, svm.predict(X_val)))   "
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 35,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
=======
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt    \n",
    "                    \n",
    "plt.plot(ccc, c_scores)\n",
<<<<<<< HEAD
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test this baby out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = [i[1][\"content\"] for i in tweets.iterrows()]\n",
    "rating_list = [i[1][\"rating\"] for i in tweets.iterrows()]"
=======
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9017657299179309\n"
     ]
    }
   ],
   "source": [
    "final = LinearSVC(tol=.000001,C=0.06)\n",
    "final.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test this baby out!"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "twitter_cleaned = preprocess_reviews(tweets_list)\n"
=======
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = [i[1][\"content\"] for i in tweets.iterrows()]\n",
    "rating_list = [i[1][\"rating\"] for i in tweets.iterrows()]"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 48,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 26,
=======
       "101"
      ]
     },
     "execution_count": 48,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "stop_words = stopwords.words('english')\n",
    "ngram_vectorizer_tweets = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
    "ngram_vectorizer_tweets.fit(twitter_cleaned)"
=======
    "twitter_cleaned = preprocess_reviews(tweets_list)\n",
    "len(twitter_cleaned)"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 49,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "(101, 138391)"
      ]
     },
     "execution_count": 27,
=======
       "(101, 138965)"
      ]
     },
     "execution_count": 49,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tws = ngram_vectorizer.transform(twitter_cleaned)\n",
<<<<<<< HEAD
    "tws.shape\n"
=======
    "tws.shape"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 50,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final.predict(tws[:100])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 51,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
=======
       "      <td>1</td>\n",
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
=======
       "      <td>0</td>\n",
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prediction  Actual\n",
       "0           0       0\n",
       "1           0       0\n",
<<<<<<< HEAD
       "2           0       1\n",
       "3           1       1\n",
       "4           1       1"
      ]
     },
     "execution_count": 29,
=======
       "2           1       1\n",
       "3           0       1\n",
       "4           1       1"
      ]
     },
     "execution_count": 51,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": rating_list[:100]}).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 52,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy: 0.72\n"
=======
      "Accuracy: 0.88\n"
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "score = 0\n",
    "for i, j in zip(predictions, rating_list):\n",
    "    total += 1\n",
    "    if i == j:\n",
    "        score += 1\n",
    "        \n",
    "print(f\"Accuracy: {score/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 53,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 54,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model_svc.pkl']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 32,
=======
     "execution_count": 54,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final, \"final_model_svc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 55,
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using trained model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    retrieve_model = joblib.load(\"final_model_svc.pkl\")\n",
    "    print(\"using trained model\")\n",
    "except:\n",
    "    print(\"model not found\")\n",
    "    joblib.dump(final, \"final_model_svc.pkl\")"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 6227eb06e1fa593f8a81c50f8155cc41435a83e0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
